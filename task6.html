<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>预训练语言模型学习之旅 (交互式笔记)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Warm Neutrals -->
    <!-- Application Structure Plan: The application is designed as a tabbed single-page application (SPA) to facilitate direct comparison between the three main pre-trained language model architectures: Encoder-Only (BERT), Decoder-Only (GPT), and Encoder-Decoder (T5). This structure was chosen over a linear report format because the primary learning goal is to understand the *differences* and respective strengths of each approach. By allowing users to switch between dedicated sections, they can actively compare architectural diagrams, pre-training task simulations, and key characteristics side-by-side. This non-linear, task-oriented design enhances comprehension and retention of complex technical concepts. -->
    <!-- Visualization & Content Choices:
        - Report Info: Core architectures of BERT, GPT, T5. Goal: Organize & Compare. Viz/Method: Interactive HTML/CSS/JS diagrams instead of static images. Interaction: Hovering over components reveals explanatory tooltips. Justification: More engaging and informative than a flat diagram, avoids SVG.
        - Report Info: Pre-training tasks (MLM, LM). Goal: Inform & Engage. Viz/Method: Interactive text simulations powered by JavaScript. Interaction: User clicks buttons to see the "Masked Language Model" and "Autoregressive Generation" processes unfold. Justification: Turns abstract concepts into concrete, memorable examples.
        - Report Info: GPT model parameter scaling. Goal: Show Change & Compare. Viz/Method: Canvas-based bar chart using Chart.js. Interaction: Hovering on bars shows exact parameter counts. Justification: Effectively visualizes the exponential growth, a key concept in the GPT lineage, using an interactive and responsive canvas element.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Noto Sans SC', sans-serif;
            background-color: #f8f7f4;
            color: #4a4a4a;
        }
        .tab-btn {
            transition: all 0.3s ease-in-out;
            border-bottom: 4px solid transparent;
        }
        .tab-btn.active {
            border-bottom-color: #4f46e5;
            color: #1e1b4b;
            transform: translateY(-2px);
        }
        .content-section {
            display: none;
        }
        .content-section.active {
            display: block;
        }
        .interactive-diagram-box {
            border: 2px dashed #d1d5db;
            background-color: #f9fafb;
            position: relative;
        }
        .diagram-component {
            transition: all 0.2s ease-in-out;
            cursor: pointer;
        }
        .diagram-component:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            z-index: 10;
        }
        .tooltip {
            display: none;
            position: absolute;
            bottom: 105%;
            left: 50%;
            transform: translateX(-50%);
            z-index: 20;
        }
        .diagram-component:hover .tooltip {
            display: block;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 40vh;
        }
    </style>
</head>
<body class="antialiased">

    <div class="container mx-auto p-4 md:p-8 max-w-5xl">

        <header class="text-center mb-8 md:mb-12">
            <h1 class="text-3xl md:text-5xl font-bold text-slate-800 mb-2">第三章：预训练语言模型</h1>
            <p class="text-lg text-slate-600">关于 Transformer 三大架构分支的交互式探索笔记</p>
        </header>

        <nav class="flex justify-center items-center border-b-2 border-slate-200 mb-8 sticky top-0 bg-f8f7f4/80 backdrop-blur-sm py-4 z-30">
            <button data-tab="bert" class="tab-btn active text-lg md:text-xl font-semibold px-4 md:px-6 py-3 text-slate-600">Encoder-Only: BERT之路</button>
            <button data-tab="gpt" class="tab-btn text-lg md:text-xl font-semibold px-4 md:px-6 py-3 text-slate-600">Decoder-Only: GPT之路</button>
            <button data-tab="t5" class="tab-btn text-lg md:text-xl font-semibold px-4 md:px-6 py-3 text-slate-600">Encoder-Decoder: T5之路</button>
        </nav>

        <main>
            <!-- BERT Section -->
            <section id="bert-content" class="content-section active animate-fade-in">
                <div class="bg-white p-6 md:p-8 rounded-2xl shadow-lg">
                    <h2 class="text-2xl md:text-3xl font-bold text-indigo-900 mb-4">BERT: 双向理解的深度探索者</h2>
                    <p class="text-slate-700 mb-6 text-base md:text-lg">BERT (Bidirectional Encoder Representations from Transformers) 的核心思想是，要真正理解一个词的含义，必须同时观察它的左侧和右侧上下文。它通过堆叠Transformer的编码器（Encoder）层来实现这一点，成为自然语言理解（NLU）任务的强大基石。</p>
                    
                    <div class="grid md:grid-cols-2 gap-8 items-start">
                        <div>
                            <h3 class="text-xl font-semibold text-slate-800 mb-3">核心架构：堆叠的编码器</h3>
                            <div class="interactive-diagram-box p-6 rounded-lg text-center">
                                <p class="mb-4 text-slate-600">将鼠标悬停在模块上以查看说明</p>
                                <div class="diagram-component bg-rose-100 text-rose-800 p-3 rounded-md mb-2 font-semibold relative">
                                    输入文本: "我爱北京[MASK]门"
                                    <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">输入序列，其中部分词被特殊符[MASK]替换。</div>
                                </div>
                                <div class="text-2xl">↓</div>
                                <div class="diagram-component bg-indigo-200 text-indigo-800 p-4 rounded-lg font-bold shadow-sm relative">
                                    多层Encoder
                                    <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">模型的核心。每个Encoder层都包含自注意力机制和前馈网络，能并行处理整个输入序列，捕捉全局上下文信息。</div>
                                </div>
                                <div class="text-2xl">↓</div>
                                <div class="diagram-component bg-emerald-100 text-emerald-800 p-3 rounded-md font-semibold relative">
                                    输出上下文表示
                                    <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">每个输入token都获得了一个富含双向上下文信息的向量表示，可用于下游任务。</div>
                                </div>
                            </div>
                        </div>

                        <div>
                            <h3 class="text-xl font-semibold text-slate-800 mb-3">预训练任务：完形填空模拟</h3>
                             <p class="text-slate-600 mb-4">BERT通过两个任务进行预训练。其中，掩码语言模型（MLM）是其精髓，就像做完形填空一样。</p>
                            <div id="mlm-demo" class="bg-slate-100 p-4 rounded-lg">
                                <p class="text-lg mb-4 text-center font-mono" id="mlm-text">学习人工智能的最好方式是亲手实践。</p>
                                <div class="flex justify-center space-x-4">
                                    <button id="mask-btn" class="bg-indigo-600 text-white font-bold py-2 px-4 rounded-lg hover:bg-indigo-700 transition">1. 随机遮盖</button>
                                    <button id="reveal-btn" class="bg-gray-600 text-white font-bold py-2 px-4 rounded-lg hover:bg-gray-700 transition">2. 揭示答案</button>
                                </div>
                            </div>
                             <div class="mt-6">
                                <h4 class="font-semibold text-slate-700">模型优势:</h4>
                                <ul class="list-disc list-inside mt-2 text-slate-600 space-y-1">
                                    <li>🧠 <strong>强大的NLU能力</strong>：对上下文的深刻双向理解使其在分类、实体识别等任务中表现优异。</li>
                                    <li>🌐 <strong>并行处理</strong>：Encoder结构允许并行计算，训练效率高。</li>
                                    <li>👎 <strong>不擅长生成</strong>：其结构不适合需要逐步生成文本的NLG任务。</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- GPT Section -->
            <section id="gpt-content" class="content-section animate-fade-in">
                 <div class="bg-white p-6 md:p-8 rounded-2xl shadow-lg">
                    <h2 class="text-2xl md:text-3xl font-bold text-cyan-900 mb-4">GPT: 自回归的语言生成大师</h2>
                    <p class="text-slate-700 mb-6 text-base md:text-lg">GPT (Generative Pre-trained Transformer) 系列模型的核心是“自回归”——像人类说话一样，逐字逐句地生成内容。它仅使用Transformer的解码器（Decoder）层，并证明了通过海量数据和模型参数的“暴力美学”，可以涌现出惊人的智能。</p>
                    
                    <div class="grid md:grid-cols-2 gap-8 items-start">
                        <div>
                            <h3 class="text-xl font-semibold text-slate-800 mb-3">核心架构：堆叠的解码器</h3>
                            <div class="interactive-diagram-box p-6 rounded-lg text-center">
                                 <p class="mb-4 text-slate-600">将鼠标悬停在模块上以查看说明</p>
                                 <div class="diagram-component bg-rose-100 text-rose-800 p-3 rounded-md mb-2 font-semibold relative">
                                    输入文本: "今天天气"
                                    <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">提供一个初始上下文或提示(Prompt)。</div>
                                </div>
                                <div class="text-2xl">↓</div>
                                <div class="diagram-component bg-cyan-200 text-cyan-800 p-4 rounded-lg font-bold shadow-sm relative">
                                    多层Decoder (带因果掩码)
                                     <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">在自注意力计算时，使用"因果掩码"，确保每个位置只能关注到它之前的信息，不能"偷看"未来，这保证了生成过程的单向性。</div>
                                </div>
                                <div class="text-2xl">↓</div>
                                <div class="diagram-component bg-emerald-100 text-emerald-800 p-3 rounded-md font-semibold relative">
                                    预测下一个词: "很"
                                    <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">模型输出对下一个词的概率分布，然后采样或选择概率最高的词。</div>
                                </div>
                            </div>
                        </div>

                        <div>
                            <h3 class="text-xl font-semibold text-slate-800 mb-3">预训练任务：预测下一个词</h3>
                            <p class="text-slate-600 mb-4">GPT的训练目标非常纯粹：给定一段文本，永远是预测下一个词是什么。</p>
                            <div id="lm-demo" class="bg-slate-100 p-4 rounded-lg">
                                <p class="text-lg mb-4 text-center font-mono" id="lm-text-container">
                                    <span>大型语言模型</span>
                                </p>
                                <div class="flex justify-center">
                                    <button id="generate-btn" class="bg-cyan-600 text-white font-bold py-2 px-4 rounded-lg hover:bg-cyan-700 transition">开始生成</button>
                                </div>
                            </div>
                        </div>
                    </div>
                     <div class="mt-8">
                         <h3 class="text-xl font-semibold text-slate-800 mb-3 text-center">进化之路：规模的胜利</h3>
                         <p class="text-slate-600 mb-4 text-center max-w-2xl mx-auto">从GPT-1到GPT-3，参数量呈指数级增长，验证了“量变引起质变”的缩放定律（Scaling Law）。</p>
                         <div class="chart-container">
                             <canvas id="gptChart"></canvas>
                         </div>
                         <div class="mt-6 text-center">
                             <h4 class="font-semibold text-slate-700">模型优势:</h4>
                             <ul class="list-disc list-inside inline-block text-left mt-2 text-slate-600 space-y-1">
                                 <li>✍️ <strong>卓越的NLG能力</strong>：天然适合所有需要生成文本的任务，如写作、对话、翻译。</li>
                                 <li>🚀 <strong>强大的少样本学习</strong>：模型规模巨大时，无需微调，仅通过少量示例（Few-shot）就能完成新任务。</li>
                                 <li>🤔 <strong>上下文理解受限</strong>：单向结构使其对全局上下文的理解不如双向模型深刻。</li>
                             </ul>
                         </div>
                     </div>
                </div>
            </section>
            
            <!-- T5 Section -->
            <section id="t5-content" class="content-section animate-fade-in">
                <div class="bg-white p-6 md:p-8 rounded-2xl shadow-lg">
                    <h2 class="text-2xl md:text-3xl font-bold text-amber-900 mb-4">T5: "万物皆可文本"的统一框架</h2>
                    <p class="text-slate-700 mb-6 text-base md:text-lg">T5 (Text-to-Text Transfer Transformer) 模型采取了“大一统”的哲学，将所有NLP任务都转化为一种“文本到文本”的格式。它使用了完整的Transformer架构，既有Encoder来理解输入，又有Decoder来生成输出，兼具两家之长。</p>
                    
                    <div class="grid md:grid-cols-2 gap-8 items-start">
                        <div>
                            <h3 class="text-xl font-semibold text-slate-800 mb-3">核心架构：完整的Transformer</h3>
                            <div class="interactive-diagram-box p-6 rounded-lg text-center">
                                 <p class="mb-4 text-slate-600">将鼠标悬停在模块上以查看说明</p>
                                 <div class="diagram-component bg-rose-100 text-rose-800 p-3 rounded-md mb-2 font-semibold relative">
                                    输入: "translate English to German: That is good."
                                     <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">所有任务都被加上一个任务前缀，然后作为文本输入。</div>
                                </div>
                                <div class="text-2xl">↓</div>
                                <div class="flex justify-around items-start">
                                    <div class="diagram-component bg-indigo-200 text-indigo-800 p-4 rounded-lg font-bold shadow-sm relative w-5/12">
                                        Encoder
                                        <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">像BERT一样，对输入文本进行双向编码，形成深度理解。</div>
                                    </div>
                                    <div class="diagram-component bg-cyan-200 text-cyan-800 p-4 rounded-lg font-bold shadow-sm relative w-5/12">
                                        Decoder
                                        <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">像GPT一样，接收Encoder的输出和已生成的部分，自回归地生成目标文本。</div>
                                    </div>
                                </div>
                                <div class="text-2xl">↓</div>
                                <div class="diagram-component bg-emerald-100 text-emerald-800 p-3 rounded-md font-semibold relative">
                                    输出: "Das ist gut."
                                     <div class="tooltip bg-slate-800 text-white text-xs rounded py-1 px-2 w-48">无论原始任务是什么，输出永远是文本。</div>
                                </div>
                            </div>
                        </div>

                        <div>
                            <h3 class="text-xl font-semibold text-slate-800 mb-3">预训练任务：统一的去噪目标</h3>
                            <p class="text-slate-600 mb-4">T5的训练任务也是一种“完形填空”，但更灵活。它会随机“破坏”输入文本（如丢掉一些片段），然后让模型学会恢复原文。</p>
                            <div class="bg-slate-100 p-4 rounded-lg">
                               <div class="font-mono text-sm">
                                   <p class="text-slate-500">原始输入:</p>
                                   <p class="mb-2">Thank you for inviting me to your party last week.</p>
                                   <p class="text-slate-500">被破坏的输入:</p>
                                   <p class="mb-2">Thank you <span class="bg-red-200 text-red-800 px-1 rounded">&lt;X&gt;</span> me to your party <span class="bg-red-200 text-red-800 px-1 rounded">&lt;Y&gt;</span> week.</p>
                                   <p class="text-slate-500">目标输出:</p>
                                   <p><span class="bg-green-200 text-green-800 px-1 rounded">&lt;X&gt;</span> for inviting <span class="bg-green-200 text-green-800 px-1 rounded">&lt;Y&gt;</span> last <span class="bg-green-200 text-green-800 px-1 rounded">&lt;Z&gt;</span></p>
                               </div>
                            </div>
                            <div class="mt-6">
                                <h4 class="font-semibold text-slate-700">模型优势:</h4>
                                <ul class="list-disc list-inside mt-2 text-slate-600 space-y-1">
                                    <li>🔄 <strong>极高的灵活性</strong>：统一的text-to-text框架可以处理各种NLP任务，无需改变模型架构。</li>
                                    <li>🤝 <strong>兼具理解与生成</strong>：结合了Encoder和Decoder的优点。</li>
                                    <li>📚 <strong>参数效率</strong>：相比同等参数量的GPT模型，在许多基准测试上表现更优。</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>
        
        <footer class="text-center mt-12 text-slate-500">
            <p>基于 happy-llm 第三章学习材料制作的学习笔记。</p>
            <p>&copy; 橙【参数乱炖】。</p>
        </footer>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const tabs = document.querySelectorAll('.tab-btn');
            const contents = document.querySelectorAll('.content-section');

            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    tabs.forEach(t => t.classList.remove('active'));
                    tab.classList.add('active');

                    contents.forEach(c => c.classList.remove('active'));
                    const activeContent = document.getElementById(tab.dataset.tab + '-content');
                    if(activeContent) {
                        activeContent.classList.add('active');
                    }
                });
            });

            // MLM Demo
            const mlmText = document.getElementById('mlm-text');
            const maskBtn = document.getElementById('mask-btn');
            const revealBtn = document.getElementById('reveal-btn');
            const originalText = "学习人工智能的最好方式是亲手实践。";
            const maskedText = "学习人工智能的最好方式是<span class='bg-red-200 text-red-800 px-2 rounded-md'>[MASK]</span>实践。";

            maskBtn.addEventListener('click', () => {
                mlmText.innerHTML = maskedText;
            });
            revealBtn.addEventListener('click', () => {
                mlmText.innerHTML = originalText;
            });

            // LM Demo
            const lmContainer = document.getElementById('lm-text-container');
            const generateBtn = document.getElementById('generate-btn');
            const wordsToGenerate = ["是", "一个", "强大", "的", "工具", "。"];
            let generateInterval;

            generateBtn.addEventListener('click', () => {
                if (generateBtn.textContent === "重新生成") {
                    lmContainer.innerHTML = '<span>大型语言模型</span>';
                }
                
                generateBtn.disabled = true;
                generateBtn.textContent = "生成中...";
                
                let i = 0;
                generateInterval = setInterval(() => {
                    if (i < wordsToGenerate.length) {
                        const newSpan = document.createElement('span');
                        newSpan.textContent = " " + wordsToGenerate[i];
                        newSpan.classList.add('animate-fade-in-word');
                        lmContainer.appendChild(newSpan);
                        i++;
                    } else {
                        clearInterval(generateInterval);
                        generateBtn.disabled = false;
                        generateBtn.textContent = "重新生成";
                    }
                }, 400);
            });

            // GPT Chart
            const gptChartCanvas = document.getElementById('gptChart');
            if(gptChartCanvas) {
                const ctx = gptChartCanvas.getContext('2d');
                new Chart(ctx, {
                    type: 'bar',
                    data: {
                        labels: ['GPT-1', 'GPT-2 (Large)', 'GPT-3'],
                        datasets: [{
                            label: '模型参数量 (对数刻度)',
                            data: [1.17e8, 1.5e9, 1.75e11],
                            backgroundColor: [
                                'rgba(56, 189, 248, 0.6)',
                                'rgba(34, 211, 238, 0.6)',
                                'rgba(16, 185, 129, 0.6)'
                            ],
                            borderColor: [
                                'rgba(56, 189, 248, 1)',
                                'rgba(34, 211, 238, 1)',
                                'rgba(16, 185, 129, 1)'
                            ],
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                type: 'logarithmic',
                                title: {
                                    display: true,
                                    text: '参数量'
                                }
                            }
                        },
                        plugins: {
                            tooltip: {
                                callbacks: {
                                    label: function(context) {
                                        let label = context.dataset.label || '';
                                        if (label) {
                                            label += ': ';
                                        }
                                        const value = context.parsed.y;
                                        if (value >= 1e9) {
                                            return label + (value / 1e9).toFixed(2) + ' B';
                                        } else if (value >= 1e6) {
                                            return label + (value / 1e6).toFixed(2) + ' M';
                                        }
                                        return label + value;
                                    }
                                }
                            }
                        }
                    }
                });
            }
            
            // CSS for fade-in animation
            const styleSheet = document.createElement("style");
            styleSheet.type = "text/css";
            styleSheet.innerText = `
                @keyframes fadeIn {
                    from { opacity: 0; transform: translateY(10px); }
                    to { opacity: 1; transform: translateY(0); }
                }
                .animate-fade-in {
                    animation: fadeIn 0.5s ease-in-out;
                }
                 @keyframes fadeInWord {
                    from { opacity: 0; }
                    to { opacity: 1; }
                }
                .animate-fade-in-word {
                    display: inline-block;
                    animation: fadeInWord 0.5s ease-in-out;
                }
            `;
            document.head.appendChild(styleSheet);
        });
    </script>
</body>
</html>
